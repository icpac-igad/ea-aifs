{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Run AIFS ENS v1 - Running 50 Ensemble Members\n",
    "\n",
    "This notebook runs ECMWF's aifs-ens-v1 data-driven model for multiple ensemble members (1-50), using ECMWF's [open data](https://www.ecmwf.int/en/forecasts/datasets/open-data) dataset and the [anemoi-inference](https://anemoi-inference.readthedocs.io/en/latest/apis/level1.html) package.\n",
    "\n",
    "aifs-ens-v1 is designed to be an inherently uncertain model, meaning that for the same initial conditions, different noise is applied within the model and a different forecast will be provided. This notebook runs all 50 perturbed ensemble members and saves each as a separate GRIB file.\n",
    "\n",
    "# 1. Install Required Packages and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Uncomment the lines below to install the required packages\n",
    "#!pip install torch==2.5.0 anemoi-inference[huggingface]==0.6.0 anemoi-models==0.6.0 anemoi-graphs==0.6.0 anemoi-datasets==0.5.23\n",
    "#!pip install earthkit-regrid==0.4.0 'ecmwf-opendata>=0.3.19'\n",
    "#!pip install flash_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import earthkit.data as ekd\n",
    "import earthkit.regrid as ekr\n",
    "\n",
    "from anemoi.inference.runners.simple import SimpleRunner\n",
    "from anemoi.inference.outputs.printer import print_state\n",
    "from anemoi.inference.outputs.gribfile import GribFileOutput\n",
    "from anemoi.inference.context import Context\n",
    "\n",
    "from ecmwf.opendata import Client as OpendataClient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Configuration and Setup\n",
    "## List of parameters to retrieve from ECMWF open data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAM_SFC = [\"10u\", \"10v\", \"2d\", \"2t\", \"msl\", \"skt\", \"sp\", \"tcw\"]\n",
    "PARAM_SFC_FC = [\"lsm\", \"z\", \"slor\", \"sdor\"]\n",
    "PARAM_SOIL = [\"sot\"]\n",
    "PARAM_PL = [\"gh\", \"t\", \"u\", \"v\", \"w\", \"q\"]\n",
    "LEVELS = [1000, 925, 850, 700, 600, 500, 400, 300, 250, 200, 150, 100, 50]\n",
    "SOIL_LEVELS = [1, 2]\n",
    "\n",
    "# Configuration for multi-run\n",
    "ENSEMBLE_MEMBERS = list(range(1, 51))  # Members 1-50\n",
    "LEAD_TIME = 72  # Hours\n",
    "OUTPUT_DIR = \"ensemble_outputs\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select a date and create output directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATE = OpendataClient(\"ecmwf\").latest()\n",
    "print(\"Initial date is\", DATE)\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "date_str = DATE.strftime(\"%Y%m%d_%H%M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Data Retrieval Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_open_data(param, levelist=[], number=None):\n",
    "    fields = defaultdict(list)\n",
    "    # Get the data for the current date and the previous date\n",
    "    for date in [DATE - datetime.timedelta(hours=6), DATE]:\n",
    "        if number is None:\n",
    "            data = ekd.from_source(\"ecmwf-open-data\", date=date, param=param, levelist=levelist)\n",
    "        else:\n",
    "            data = ekd.from_source(\"ecmwf-open-data\", date=date, param=param, levelist=levelist, \n",
    "                                 number=[number], stream='enfo')\n",
    "        \n",
    "        for f in data:\n",
    "            # Open data is between -180 and 180, we need to shift it to 0-360\n",
    "            assert f.to_numpy().shape == (721, 1440)\n",
    "            values = np.roll(f.to_numpy(), -f.shape[1] // 2, axis=1)\n",
    "            # Interpolate the data from 0.25 to N320\n",
    "            values = ekr.interpolate(values, {\"grid\": (0.25, 0.25)}, {\"grid\": \"N320\"})\n",
    "            # Add the values to the list\n",
    "            name = f\"{f.metadata('param')}_{f.metadata('levelist')}\" if levelist else f.metadata(\"param\")\n",
    "            fields[name].append(values)\n",
    "\n",
    "    # Create a single matrix for each parameter\n",
    "    for param, values in fields.items():\n",
    "        fields[param] = np.stack(values)\n",
    "\n",
    "    return fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Function to Get Input Fields for a Given Ensemble Member"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_fields(number):\n",
    "    \"\"\"Get input fields for a specific ensemble member.\"\"\"\n",
    "    fields = {}\n",
    "    \n",
    "    # Add single level fields\n",
    "    fields.update(get_open_data(param=PARAM_SFC, number=number))\n",
    "    fields.update(get_open_data(param=PARAM_SFC_FC))  # Constant fields\n",
    "    \n",
    "    # Add soil fields\n",
    "    soil = get_open_data(param=PARAM_SOIL, levelist=SOIL_LEVELS, number=number)\n",
    "    \n",
    "    # Rename soil parameters\n",
    "    mapping = {'sot_1': 'stl1', 'sot_2': 'stl2',\n",
    "               'vsw_1': 'swvl1', 'vsw_2': 'swvl2'}\n",
    "    for k, v in soil.items():\n",
    "        fields[mapping[k]] = v\n",
    "    \n",
    "    # Add pressure level fields\n",
    "    fields.update(get_open_data(param=PARAM_PL, levelist=LEVELS, number=number))\n",
    "    \n",
    "    # Convert geopotential height to geopotential\n",
    "    for level in LEVELS:\n",
    "        gh = fields.pop(f\"gh_{level}\")\n",
    "        fields[f\"z_{level}\"] = gh * 9.80665\n",
    "    \n",
    "    return fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Load the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Set environment variables to reduce memory usage\n",
    "# import os\n",
    "# os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True' \n",
    "# os.environ['ANEMOI_INFERENCE_NUM_CHUNKS'] = '16'\n",
    "\n",
    "checkpoint = {\"huggingface\": \"ecmwf/aifs-ens-1.0\"}\n",
    "runner = SimpleRunner(checkpoint, device=\"cuda\")\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Run Forecasts for All Ensemble Members"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track processing times\n",
    "processing_times = []\n",
    "\n",
    "for member in ENSEMBLE_MEMBERS:\n",
    "    start_time = time.time()\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Processing ensemble member {member}/{len(ENSEMBLE_MEMBERS)}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    try:\n",
    "        # Get input fields for this member\n",
    "        print(f\"Retrieving initial conditions for member {member}...\")\n",
    "        fields = get_input_fields(member)\n",
    "        input_state = dict(date=DATE, fields=fields)\n",
    "        \n",
    "        # Setup output file\n",
    "        grib_file = f\"{OUTPUT_DIR}/aifs_ens_forecast_{date_str}_member{member:03d}.grib\"\n",
    "        \n",
    "        # Create context for outputs\n",
    "        context = Context()\n",
    "        context.time_step = 6  # 6-hour time step\n",
    "        context.lead_time = LEAD_TIME\n",
    "        context.reference_date = DATE\n",
    "        \n",
    "        # Initialize GRIB output\n",
    "        grib_output = GribFileOutput(context, path=grib_file)\n",
    "        \n",
    "        # Run forecast\n",
    "        print(f\"Running forecast for member {member}...\")\n",
    "        outputs_initialized = False\n",
    "        step_count = 0\n",
    "        \n",
    "        for state in runner.run(input_state=input_state, lead_time=LEAD_TIME):\n",
    "            # Initialize output on first state\n",
    "            if not outputs_initialized:\n",
    "                grib_output.open(state)\n",
    "                outputs_initialized = True\n",
    "            \n",
    "            # Write to output\n",
    "            grib_output.write_step(state)\n",
    "            step_count += 1\n",
    "            \n",
    "            # Print progress every 4 steps (24 hours)\n",
    "            if step_count % 4 == 0:\n",
    "                print(f\"  Member {member}: {step_count * 6} hours completed\")\n",
    "        \n",
    "        # Close output\n",
    "        grib_output.close()\n",
    "        \n",
    "        # Verify file\n",
    "        if os.path.exists(grib_file):\n",
    "            file_size = os.path.getsize(grib_file) / (1024 * 1024)  # Size in MB\n",
    "            print(f\"✓ Member {member} completed: {grib_file} ({file_size:.2f} MB)\")\n",
    "        else:\n",
    "            print(f\"✗ Error: Output file not created for member {member}\")\n",
    "        \n",
    "        # Track time\n",
    "        member_time = time.time() - start_time\n",
    "        processing_times.append(member_time)\n",
    "        print(f\"Member {member} processing time: {member_time:.2f} seconds\")\n",
    "        \n",
    "        # Estimate remaining time\n",
    "        if len(processing_times) > 0:\n",
    "            avg_time = sum(processing_times) / len(processing_times)\n",
    "            remaining_members = len(ENSEMBLE_MEMBERS) - member\n",
    "            est_remaining = avg_time * remaining_members\n",
    "            print(f\"Estimated time remaining: {est_remaining/60:.1f} minutes\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error processing member {member}: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"All ensemble members processed!\")\n",
    "print(f\"Total processing time: {sum(processing_times)/60:.1f} minutes\")\n",
    "print(f\"Average time per member: {sum(processing_times)/len(processing_times):.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Verify Output Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all generated files\n",
    "grib_files = sorted([f for f in os.listdir(OUTPUT_DIR) if f.endswith('.grib')])\n",
    "print(f\"\\nGenerated {len(grib_files)} GRIB files:\")\n",
    "\n",
    "total_size = 0\n",
    "for f in grib_files:\n",
    "    file_path = os.path.join(OUTPUT_DIR, f)\n",
    "    file_size = os.path.getsize(file_path) / (1024 * 1024)  # MB\n",
    "    total_size += file_size\n",
    "    print(f\"  {f}: {file_size:.2f} MB\")\n",
    "\n",
    "print(f\"\\nTotal disk space used: {total_size:.2f} MB ({total_size/1024:.2f} GB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Optional: Quick Verification of a Sample File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify one of the output files\n",
    "if grib_files:\n",
    "    sample_file = os.path.join(OUTPUT_DIR, grib_files[0])\n",
    "    print(f\"\\nChecking sample file: {grib_files[0]}\")\n",
    "    \n",
    "    try:\n",
    "        grib_data = ekd.from_source(\"file\", sample_file)\n",
    "        print(f\"File contains {len(grib_data)} fields\")\n",
    "        \n",
    "        # Show first few fields\n",
    "        print(\"\\nFirst 5 fields:\")\n",
    "        for i, field in enumerate(grib_data[:5]):\n",
    "            meta = field.metadata()\n",
    "            print(f\"  {i+1}: {meta.get('param')} at {meta.get('levelist', 'surface')} - \"\n",
    "                  f\"step: {meta.get('step')} hours\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}